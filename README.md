# Group-Relative Policy Optimization(GRPO) for Business Decision Systems
Critic-Free Reinforcement Learning with Clustering and Attention for Personalized Promotions
This paper builds on my earlier study, "Exploring Clustered Optimal Policies via Off-Policy Reinforcement Learning for Business Use Cases", where I compared single-head and clustered reinforcement learning (RL) models for personalized promotions using offline customer logs and inverse propensity scoring.
Traditional RL methods like PPO rely heavily on critic networks, which add computational overhead and can destabilize training. Inspired by recent progress in large language model alignment (e.g., DeepSeek-LM), I explore Group-Relative Policy Optimization (GRPO) - a critic-free alternative that uses simple group-and-action baselines to reduce variance and speed up convergence. GRPO's simplicity leads to faster learning and lower resource usage, making it especially attractive for practical business systems.
I explore whether GRPO, originally successful in language tasks, can also help solve real-world business problems. In business, companies often juggle several goals at once - like increasing profit, maintaining fairness, and preserving brand tone - but still need to make decisions using a single policy. GRPO is a natural fit: it treats each goal or customer segment as its own "group" and learns them all at once, within a stable and simple framework that doesn't rely on critics or manually tuned reward weights.
This paper has three goals. First, I explain GRPO in business-friendly terms, positioning it as a lightweight alternative to PPO or A2C when grouped data or clustering is available. Second, I apply it to a real use case - coupon policy personalization - where grouped baselines clearly drive profit gains. Third, I extend GRPO in two directions:
Cluster-Adaptive GRPO, which pairs multiple expert heads with a lightweight gating network, boosting IPS-estimated ROI from 0.08 to 0.96.
Transformer-GRPO, which swaps in a compact state–action transformer for richer representations, raising IPS profit to 3.78 while maintaining GRPO's stability.

Using the same dataset and evaluation setup throughout, we isolate the effect of each improvement. The remainder of this paper moves from theory to implementation and concludes with guidance on when clustering or transformers are worth the extra complexity in business RL systems.
